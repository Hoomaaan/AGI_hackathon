{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "This notebook lets you run the training and inference on the local EC2 instance instead of the other notebooks that use SageMaker.\n",
    "\n",
    "# Install requirements\n",
    "\n",
    "We'll use the same files as the SageMaker training, so we'll first move to the assets directory and run our scripts from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/environment/FineTuning/HuggingFaceExample/01_finetuning/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ubuntu/environment/FineTuning/HuggingFaceExample/01_finetuning/assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting optimum-neuron==0.3.0 (from -r requirements.txt (line 1))\n",
      "  Downloading optimum_neuron-0.3.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting peft==0.16.0 (from -r requirements.txt (line 2))\n",
      "  Downloading peft-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting trl==0.11.4 (from -r requirements.txt (line 3))\n",
      "  Downloading trl-0.11.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting huggingface_hub==0.33.4 (from -r requirements.txt (line 4))\n",
      "  Downloading huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting datasets==3.6.0 (from -r requirements.txt (line 5))\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: transformers~=4.51.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from optimum-neuron==0.3.0->-r requirements.txt (line 1)) (4.51.3)\n",
      "Collecting accelerate==1.8.1 (from optimum-neuron==0.3.0->-r requirements.txt (line 1))\n",
      "  Downloading accelerate-1.8.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting optimum~=1.24.0 (from optimum-neuron==0.3.0->-r requirements.txt (line 1))\n",
      "  Downloading optimum-1.24.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting numpy<=1.25.2,>=1.22.2 (from optimum-neuron==0.3.0->-r requirements.txt (line 1))\n",
      "  Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting protobuf<4,>=3.20.3 (from optimum-neuron==0.3.0->-r requirements.txt (line 1))\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from peft==0.16.0->-r requirements.txt (line 2)) (25.0)\n",
      "Requirement already satisfied: psutil in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from peft==0.16.0->-r requirements.txt (line 2)) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from peft==0.16.0->-r requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from peft==0.16.0->-r requirements.txt (line 2)) (2.7.1)\n",
      "Requirement already satisfied: tqdm in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from peft==0.16.0->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: safetensors in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from peft==0.16.0->-r requirements.txt (line 2)) (0.5.3)\n",
      "Collecting tyro>=0.5.11 (from trl==0.11.4->-r requirements.txt (line 3))\n",
      "  Downloading tyro-0.9.32-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filelock in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from huggingface_hub==0.33.4->-r requirements.txt (line 4)) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from huggingface_hub==0.33.4->-r requirements.txt (line 4)) (2025.7.0)\n",
      "Requirement already satisfied: requests in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from huggingface_hub==0.33.4->-r requirements.txt (line 4)) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from huggingface_hub==0.33.4->-r requirements.txt (line 4)) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from huggingface_hub==0.33.4->-r requirements.txt (line 4)) (1.1.5)\n",
      "Collecting pyarrow>=15.0.0 (from datasets==3.6.0->-r requirements.txt (line 5))\n",
      "  Downloading pyarrow-21.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==3.6.0->-r requirements.txt (line 5))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from datasets==3.6.0->-r requirements.txt (line 5)) (2.3.1)\n",
      "Collecting xxhash (from datasets==3.6.0->-r requirements.txt (line 5))\n",
      "  Downloading xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets==3.6.0->-r requirements.txt (line 5))\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub==0.33.4->-r requirements.txt (line 4))\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 5)) (3.12.15)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from transformers~=4.51.0->optimum-neuron==0.3.0->-r requirements.txt (line 1)) (2025.7.34)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from transformers~=4.51.0->optimum-neuron==0.3.0->-r requirements.txt (line 1)) (0.21.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 5)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 5)) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 5)) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 5)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 5)) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 5)) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 5)) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 5)) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 5)) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from requests->huggingface_hub==0.33.4->-r requirements.txt (line 4)) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from requests->huggingface_hub==0.33.4->-r requirements.txt (line 4)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from requests->huggingface_hub==0.33.4->-r requirements.txt (line 4)) (2025.7.14)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from triton==3.3.1->torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (1.3.0)\n",
      "Collecting docstring-parser>=0.15 (from tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 3))\n",
      "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 3)) (14.1.0)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 3))\n",
      "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 3))\n",
      "  Downloading typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 3)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 3)) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 3)) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from pandas->datasets==3.6.0->-r requirements.txt (line 5)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from pandas->datasets==3.6.0->-r requirements.txt (line 5)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from pandas->datasets==3.6.0->-r requirements.txt (line 5)) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==3.6.0->-r requirements.txt (line 5)) (1.17.0)\n",
      "Downloading optimum_neuron-0.3.0-py3-none-any.whl (470 kB)\n",
      "Downloading peft-0.16.0-py3-none-any.whl (472 kB)\n",
      "Downloading trl-0.11.4-py3-none-any.whl (316 kB)\n",
      "Downloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading accelerate-1.8.1-py3-none-any.whl (365 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Downloading optimum-1.24.0-py3-none-any.whl (433 kB)\n",
      "Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-21.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.7/42.7 MB\u001b[0m \u001b[31m268.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.32-py3-none-any.whl (132 kB)\n",
      "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Downloading typeguard-4.4.4-py3-none-any.whl (34 kB)\n",
      "Downloading xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: xxhash, typeguard, shtab, pyarrow, protobuf, numpy, fsspec, docstring-parser, dill, multiprocess, huggingface_hub, tyro, accelerate, peft, optimum, datasets, trl, optimum-neuron\n",
      "\u001b[2K  Attempting uninstall: protobufmâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/18\u001b[0m [pyarrow]\n",
      "\u001b[2K    Found existing installation: protobuf 6.31.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/18\u001b[0m [pyarrow]\n",
      "\u001b[2K    Uninstalling protobuf-6.31.1:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/18\u001b[0m [pyarrow]\n",
      "\u001b[2K      Successfully uninstalled protobuf-6.31.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/18\u001b[0m [pyarrow]\n",
      "\u001b[2K  Attempting uninstall: numpyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/18\u001b[0m [pyarrow]\n",
      "\u001b[2K    Found existing installation: numpy 1.26.4â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/18\u001b[0m [pyarrow]\n",
      "\u001b[2K    Uninstalling numpy-1.26.4:[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/18\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled numpy-1.26.4â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/18\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: fsspec[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/18\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.7.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/18\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling fsspec-2025.7.0:mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/18\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.7.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/18\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: dill[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 7/18\u001b[0m [docstring-parser]\n",
      "\u001b[2K    Found existing installation: dill 0.4.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 7/18\u001b[0m [docstring-parser]\n",
      "\u001b[2K    Uninstalling dill-0.4.0:â•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 7/18\u001b[0m [docstring-parser]\n",
      "\u001b[2K      Successfully uninstalled dill-0.4.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 7/18\u001b[0m [docstring-parser]\n",
      "\u001b[2K  Attempting uninstall: huggingface_hubâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 7/18\u001b[0m [docstring-parser]\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.34.3â”â”â”â”â”â”â”\u001b[0m \u001b[32m 7/18\u001b[0m [docstring-parser]\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.34.3:\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/18\u001b[0m [huggingface_hub]\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.34.3â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/18\u001b[0m [huggingface_hub]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18/18\u001b[0m [optimum-neuron]m [optimum-neuron]]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.8.1 datasets-3.6.0 dill-0.3.8 docstring-parser-0.17.0 fsspec-2025.3.0 huggingface_hub-0.33.4 multiprocess-0.70.16 numpy-1.25.2 optimum-1.24.0 optimum-neuron-0.3.0 peft-0.16.0 protobuf-3.20.3 pyarrow-21.0.0 shtab-1.7.2 trl-0.11.4 typeguard-4.4.4 tyro-0.9.32 xxhash-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We will use the same training scripts as we do in the SageMaker examples, we just need to launch them with the torchrun process and the same parameters that we would have passed in.  See the Finetune-TinyLlama-1.1B notebook for more information on the parameters.\n",
    "\n",
    "Additionally, this example uses a Qwen model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1004 21:09:31.015000 30300 torch/distributed/run.py:766] \n",
      "W1004 21:09:31.015000 30300 torch/distributed/run.py:766] *****************************************\n",
      "W1004 21:09:31.015000 30300 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1004 21:09:31.015000 30300 torch/distributed/run.py:766] *****************************************\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "2025-10-04 21:09:49.213775: W neuron/nrt_adaptor.cc:53] nrt_tensor_write_hugepage() is not available, will fall back to nrt_tensor_write().\n",
      "2025-10-04 21:09:49.213778: W neuron/nrt_adaptor.cc:53] nrt_tensor_write_hugepage() is not available, will fall back to nrt_tensor_write().\n",
      "2025-10-04 21:09:49.213811: W neuron/nrt_adaptor.cc:62] nrt_tensor_read_hugepage() is not available, will fall back to nrt_tensor_read().\n",
      "2025-10-04 21:09:49.213813: W neuron/nrt_adaptor.cc:62] nrt_tensor_read_hugepage() is not available, will fall back to nrt_tensor_read().\n",
      "2025-Oct-04 21:09:49.0223 30322:30389 [1] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):213 CCOM WARN NET/OFI Failed to initialize sendrecv protocol\n",
      "2025-Oct-04 21:09:49.0223 30321:30386 [0] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):213 CCOM WARN NET/OFI Failed to initialize sendrecv protocol\n",
      "2025-Oct-04 21:09:49.0224 30322:30389 [1] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):354 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2025-Oct-04 21:09:49.0226 30321:30386 [0] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):354 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2025-Oct-04 21:09:49.0226 30321:30386 [0] ncclResult_t nccl_net_ofi_init_no_atexit_fini_v6(ncclDebugLogger_t):183 CCOM WARN NET/OFI Initializing plugin failed\n",
      "2025-Oct-04 21:09:49.0227 30322:30389 [1] ncclResult_t nccl_net_ofi_init_no_atexit_fini_v6(ncclDebugLogger_t):183 CCOM WARN NET/OFI Initializing plugin failed\n",
      "2025-Oct-04 21:09:49.0228 30321:30386 [0] net_plugin.cc:97 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n",
      "2025-Oct-04 21:09:49.0230 30322:30389 [1] net_plugin.cc:97 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n",
      "[2025-10-04 21:09:49.270: I neuronx_distributed/parallel_layers/parallel_state.py:628] > initializing tensor model parallel with size 2\n",
      "[2025-10-04 21:09:49.271: I neuronx_distributed/parallel_layers/parallel_state.py:629] > initializing pipeline model parallel with size 1\n",
      "[2025-10-04 21:09:49.271: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing context model parallel with size 1\n",
      "[2025-10-04 21:09:49.271: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing data parallel with size 1\n",
      "[2025-10-04 21:09:49.271: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing world size to 2\n",
      "2025-10-04 21:09:49.000360:  30321  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --framework=XLA /tmp/ubuntu/neuroncc_compile_workdir/354249d2-8f20-4a1d-9c9e-0c422d5317ba/model.MODULE_910320559639473753+e30acd3a.hlo_module.pb --output /tmp/ubuntu/neuroncc_compile_workdir/354249d2-8f20-4a1d-9c9e-0c422d5317ba/model.MODULE_910320559639473753+e30acd3a.neff --target=trn1 --verbose=35\n",
      ".Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "[2025-10-04 21:09:52.665: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x71b1dc6a9240>, 'Ascending Ring PG Group')>\n",
      "[2025-10-04 21:09:52.666: I neuronx_distributed/parallel_layers/parallel_state.py:668] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1]]\n",
      "[2025-10-04 21:09:52.666: I neuronx_distributed/parallel_layers/parallel_state.py:669] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1]]\n",
      "[2025-10-04 21:09:52.666: I neuronx_distributed/parallel_layers/parallel_state.py:670] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1]]\n",
      "[2025-10-04 21:09:52.666: I neuronx_distributed/parallel_layers/parallel_state.py:671] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1]]\n",
      "[2025-10-04 21:09:52.667: I neuronx_distributed/parallel_layers/parallel_state.py:672] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1]]\n",
      "[2025-10-04 21:09:52.667: I neuronx_distributed/parallel_layers/parallel_state.py:673] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1]]\n",
      "torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "No Hugging Face token found in environment, checking AWS Secrets Manager...\n",
      "Logging in to Hugging Face Hub...\n",
      "README.md: 4.43kB [00:00, 31.4MB/s]\n",
      "2025-10-04 21:09:54.000362:  30322  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_910320559639473753+e30acd3a/model.neff\n",
      "torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "No Hugging Face token found in environment, checking AWS Secrets Manager...\n",
      "Logging in to Hugging Face Hub...\n",
      "sql_create_context_v4.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21.8M/21.8M [00:00<00:00, 33.3MB/s]\n",
      "Generating train split: 100%|â–ˆâ–ˆ| 78577/78577 [00:00<00:00, 387882.29 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:03<00:00, 14804.96 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:03<00:00, 14674.04 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 13879.88 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 13936.23 examples/s]\n",
      "tokenizer_config.json: 9.73kB [00:00, 52.1MB/s]\n",
      "vocab.json: 2.78MB [00:00, 88.9MB/s]\n",
      "merges.txt: 1.67MB [00:00, 171MB/s]\n",
      "tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11.4M/11.4M [00:00<00:00, 26.6MB/s]\n",
      "config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 726/726 [00:00<00:00, 7.32MB/s]\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/kvcache/kv_cache_manager.py:24: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..attention.gqa import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/models/inference/llama/modeling_llama.py:45: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..backend.modules.attention.attention_base import NeuronAttentionBase\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/kvcache/kv_cache_manager.py:24: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..attention.gqa import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/models/inference/llama/modeling_llama.py:45: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..backend.modules.attention.attention_base import NeuronAttentionBase\n",
      "model.safetensors.index.json: 25.6kB [00:00, 139MB/s]\n",
      "Fetching 2 files:   0%|                                   | 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-00002.safetensors:   0%|             | 0.00/3.44G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   0%|              | 0.00/622M [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   0%|      | 68.9k/622M [00:00<56:50, 182kB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   3%|â–    | 18.9M/622M [00:00<00:15, 37.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|     | 853k/3.44G [00:00<54:07, 1.06MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|    | 20.0M/3.44G [00:01<02:30, 22.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|â–    | 98.4M/3.44G [00:01<00:26, 127MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  14%|â–‹    | 86.0M/622M [00:01<00:06, 87.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|â–     | 166M/3.44G [00:01<00:17, 193MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  35%|â–ˆâ–ˆâ–    | 220M/622M [00:01<00:01, 235MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|â–     | 202M/3.44G [00:01<00:15, 206MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  57%|â–ˆâ–ˆâ–ˆâ–‰   | 354M/622M [00:01<00:00, 377MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 488M/622M [00:01<00:00, 533MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|â–     | 265M/3.44G [00:01<00:11, 273MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 622M/622M [00:01<00:00, 365MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  11%|â–‹     | 371M/3.44G [00:01<00:07, 403MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|â–‹     | 424M/3.44G [00:01<00:07, 398MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|â–Š     | 484M/3.44G [00:02<00:06, 428MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|â–‰     | 542M/3.44G [00:02<00:06, 462MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|â–ˆ     | 623M/3.44G [00:02<00:06, 418MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|â–ˆâ–    | 677M/3.44G [00:02<00:08, 345MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|â–ˆâ–    | 744M/3.44G [00:04<00:24, 108MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|â–ˆâ–   | 823M/3.44G [00:05<00:28, 92.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|â–ˆâ–   | 922M/3.44G [00:08<00:49, 50.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|â–ˆâ–   | 966M/3.44G [00:08<00:42, 58.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|â–ˆâ–   | 991M/3.44G [00:09<00:39, 61.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|â–ˆâ–  | 1.02G/3.44G [00:09<00:40, 59.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|â–ˆâ–  | 1.07G/3.44G [00:09<00:29, 80.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|â–ˆâ–  | 1.09G/3.44G [00:10<00:31, 73.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|â–ˆâ–  | 1.12G/3.44G [00:10<00:29, 79.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|â–ˆâ–  | 1.16G/3.44G [00:11<00:30, 73.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|â–ˆâ–  | 1.17G/3.44G [00:11<00:30, 73.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|â–ˆâ–  | 1.20G/3.44G [00:11<00:27, 82.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|â–ˆâ–  | 1.24G/3.44G [00:11<00:23, 91.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|â–ˆâ–Š   | 1.28G/3.44G [00:12<00:18, 117MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|â–ˆâ–Œ  | 1.33G/3.44G [00:12<00:21, 96.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|â–ˆâ–Œ  | 1.38G/3.44G [00:13<00:22, 90.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|â–ˆâ–‹  | 1.42G/3.44G [00:13<00:22, 87.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|â–ˆâ–‹  | 1.48G/3.44G [00:14<00:22, 87.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|â–ˆâ–Š  | 1.55G/3.44G [00:15<00:21, 89.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|â–ˆâ–‰  | 1.62G/3.44G [00:17<00:29, 61.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|â–ˆâ–‰  | 1.68G/3.44G [00:17<00:24, 72.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|â–ˆâ–‰  | 1.71G/3.44G [00:17<00:20, 85.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|â–ˆâ–ˆâ–Œ  | 1.77G/3.44G [00:17<00:14, 119MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|â–ˆâ–ˆâ–‹  | 1.81G/3.44G [00:17<00:11, 142MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|â–ˆâ–ˆâ–‹  | 1.87G/3.44G [00:18<00:12, 129MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|â–ˆâ–ˆâ–‰  | 2.05G/3.44G [00:18<00:06, 200MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|â–ˆâ–ˆâ–ˆâ– | 2.18G/3.44G [00:20<00:09, 132MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|â–ˆâ–ˆâ–ˆâ– | 2.21G/3.44G [00:20<00:09, 130MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|â–ˆâ–ˆâ–ˆâ– | 2.24G/3.44G [00:21<00:09, 126MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|â–ˆâ–ˆâ–ˆâ– | 2.29G/3.44G [00:21<00:07, 156MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|â–ˆâ–ˆâ–ˆâ– | 2.34G/3.44G [00:21<00:07, 142MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–Œ | 2.45G/3.44G [00:21<00:05, 197MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–Š | 2.62G/3.44G [00:22<00:03, 242MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–‰ | 2.75G/3.44G [00:22<00:02, 294MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆ | 2.80G/3.44G [00:22<00:02, 281MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆ | 2.84G/3.44G [00:23<00:02, 223MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 2.91G/3.44G [00:23<00:02, 211MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.03G/3.44G [00:23<00:01, 276MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.09G/3.44G [00:24<00:01, 241MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.16G/3.44G [00:25<00:02, 109MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.20G/3.44G [00:26<00:02, 110MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.31G/3.44G [00:26<00:00, 133MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.44G/3.44G [00:28<00:00, 121MB/s]\u001b[A\n",
      "Fetching 2 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:28<00:00, 14.30s/it]\n",
      "Fetching 2 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:28<00:00, 14.30s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.22it/s]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.22it/s]\n",
      "torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/transformers/training_args.py:2058: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/transformers/training_args.py:2058: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "Generating train split: 5532 examples [00:11, 471.03 examples/s]\n",
      "Generating train split: 55 examples [00:00, 488.18 examples/s]\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/trainers.py:1467: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `NeuronSFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer.__init__(self, *args, **kwargs)\n",
      "No label_names provided for model class `NeuronPeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/trainers.py:1467: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `NeuronSFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer.__init__(self, *args, **kwargs)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/trainers.py:1726: UserWarning: You passed `packing=True` to the SFTTrainer/SFTConfig, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `NeuronPeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/trainers.py:1726: UserWarning: You passed `packing=True` to the SFTTrainer/SFTConfig, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5,532\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Total optimization steps = 1,000\n",
      "  Number of trainable parameters = 16,515,072\n",
      "model.neff: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11.3k/11.3k [00:00<00:00, 92.6MB/s]\n",
      "2025-10-04 21:10:56.000788:  30321  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_14152517080381013961+a3455b04/model.neff\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "2025-10-04 21:10:56.000848:  30322  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_14152517080381013961+a3455b04/model.neff\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:485: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:485: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_13(â€¦): 100%|â–ˆ| 21.4M/21.4M [00:00<00:00, 3\n",
      "2025-10-04 21:11:00.000509:  30321  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_13340102934497073113+a3455b04/model.neff\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_14(â€¦): 100%|â–ˆ| 21.4M/21.4M [00:00<00:00, 2\n",
      "2025-10-04 21:11:00.000673:  30322  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_14770574952780443587+a3455b04/model.neff\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_63(â€¦): 100%|â–ˆ| 21.4M/21.4M [00:00<00:00, 4\n",
      "2025-10-04 21:11:06.000987:  30321  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_6394509931854585423+a3455b04/model.neff\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_44(â€¦): 100%|â–ˆ| 21.3M/21.3M [00:00<00:00, 3\n",
      "2025-10-04 21:11:07.000132:  30322  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_4493465029485591167+a3455b04/model.neff\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_17(â€¦): 100%|â–ˆ| 15.6M/15.6M [00:00<00:00, 3\n",
      "2025-10-04 21:11:16.000735:  30321  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_17239445601988651504+a3455b04/model.neff\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_53(â€¦): 100%|â–ˆ| 16.4M/16.4M [00:00<00:00, 3\n",
      "2025-10-04 21:11:16.000750:  30322  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_5391683354399705816+a3455b04/model.neff\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_31(â€¦): 100%|â–ˆ| 1.51M/1.51M [00:00<00:00, 4\n",
      "2025-10-04 21:11:24.000255:  30321  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_3188978248012181906+a3455b04/model.neff\n",
      "2025-10-04 21:11:24.000304:  30322  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_3188978248012181906+a3455b04/model.neff\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_91(â€¦): 100%|â–ˆ| 1.98M/1.98M [00:00<00:00, 4\n",
      "2025-10-04 21:11:28.000145:  30322  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_9123409164792130896+a3455b04/model.neff\n",
      "2025-10-04 21:11:28.000147:  30321  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_9123409164792130896+a3455b04/model.neff\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_96(â€¦): 100%|â–ˆ| 1.98M/1.98M [00:00<00:00, 5\n",
      "2025-10-04 21:11:38.000827:  30322  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_9654582282420716943+a3455b04/model.neff\n",
      "2025-10-04 21:11:38.000875:  30321  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_9654582282420716943+a3455b04/model.neff\n",
      "{'loss': 1.7589, 'learning_rate': 4.9500000000000004e-05, 'grad_norm': 1.375, 'epoch': 0.0036153289949385392}\n",
      "{'loss': 1.1141, 'learning_rate': 4.9e-05, 'grad_norm': 0.55078125, 'epoch': 0.0072306579898770785}\n",
      "{'loss': 0.9875, 'learning_rate': 4.85e-05, 'grad_norm': 0.41796875, 'epoch': 0.010845986984815618}\n",
      "{'loss': 0.8968, 'learning_rate': 4.8e-05, 'grad_norm': 0.376953125, 'epoch': 0.014461315979754157}\n",
      "{'loss': 0.8117, 'learning_rate': 4.75e-05, 'grad_norm': 0.33984375, 'epoch': 0.018076644974692697}\n",
      "{'loss': 0.8036, 'learning_rate': 4.7e-05, 'grad_norm': 0.455078125, 'epoch': 0.021691973969631236}\n",
      "{'loss': 0.7604, 'learning_rate': 4.6500000000000005e-05, 'grad_norm': 0.4140625, 'epoch': 0.025307302964569775}\n",
      "{'loss': 0.7523, 'learning_rate': 4.600000000000001e-05, 'grad_norm': 0.44140625, 'epoch': 0.028922631959508314}\n",
      "{'loss': 0.734, 'learning_rate': 4.55e-05, 'grad_norm': 0.48046875, 'epoch': 0.03253796095444685}\n",
      "{'loss': 0.6802, 'learning_rate': 4.5e-05, 'grad_norm': 0.4765625, 'epoch': 0.036153289949385395}\n",
      "{'loss': 0.69, 'learning_rate': 4.4500000000000004e-05, 'grad_norm': 0.49609375, 'epoch': 0.03976861894432393}\n",
      "{'loss': 0.6929, 'learning_rate': 4.4000000000000006e-05, 'grad_norm': 0.5, 'epoch': 0.04338394793926247}\n",
      "{'loss': 0.6719, 'learning_rate': 4.35e-05, 'grad_norm': 0.470703125, 'epoch': 0.046999276934201015}\n",
      "{'loss': 0.6586, 'learning_rate': 4.3e-05, 'grad_norm': 0.46484375, 'epoch': 0.05061460592913955}\n",
      "{'loss': 0.6836, 'learning_rate': 4.25e-05, 'grad_norm': 0.53125, 'epoch': 0.05422993492407809}\n",
      "{'loss': 0.7023, 'learning_rate': 4.2e-05, 'grad_norm': 0.5546875, 'epoch': 0.05784526391901663}\n",
      "{'loss': 0.6744, 'learning_rate': 4.15e-05, 'grad_norm': 0.4765625, 'epoch': 0.06146059291395517}\n",
      "{'loss': 0.6754, 'learning_rate': 4.1e-05, 'grad_norm': 0.49609375, 'epoch': 0.0650759219088937}\n",
      "{'loss': 0.6554, 'learning_rate': 4.05e-05, 'grad_norm': 0.478515625, 'epoch': 0.06869125090383225}\n",
      "{'loss': 0.6459, 'learning_rate': 4e-05, 'grad_norm': 0.484375, 'epoch': 0.07230657989877079}\n",
      "{'loss': 0.6305, 'learning_rate': 3.9500000000000005e-05, 'grad_norm': 0.55859375, 'epoch': 0.07592190889370933}\n",
      "{'loss': 0.6645, 'learning_rate': 3.9000000000000006e-05, 'grad_norm': 0.80859375, 'epoch': 0.07953723788864786}\n",
      "{'loss': 0.6399, 'learning_rate': 3.85e-05, 'grad_norm': 0.5, 'epoch': 0.08315256688358641}\n",
      "{'loss': 0.6122, 'learning_rate': 3.8e-05, 'grad_norm': 0.55859375, 'epoch': 0.08676789587852494}\n",
      "{'loss': 0.6024, 'learning_rate': 3.7500000000000003e-05, 'grad_norm': 0.52734375, 'epoch': 0.09038322487346348}\n",
      "{'loss': 0.6132, 'learning_rate': 3.7e-05, 'grad_norm': 0.546875, 'epoch': 0.09399855386840203}\n",
      "{'loss': 0.6194, 'learning_rate': 3.65e-05, 'grad_norm': 0.5, 'epoch': 0.09761388286334056}\n",
      "{'loss': 0.6223, 'learning_rate': 3.6e-05, 'grad_norm': 0.55859375, 'epoch': 0.1012292118582791}\n",
      "{'loss': 0.6218, 'learning_rate': 3.55e-05, 'grad_norm': 0.5859375, 'epoch': 0.10484454085321765}\n",
      "{'loss': 0.6482, 'learning_rate': 3.5e-05, 'grad_norm': 0.53125, 'epoch': 0.10845986984815618}\n",
      "{'loss': 0.598, 'learning_rate': 3.45e-05, 'grad_norm': 0.5234375, 'epoch': 0.11207519884309472}\n",
      "{'loss': 0.6297, 'learning_rate': 3.4000000000000007e-05, 'grad_norm': 0.62109375, 'epoch': 0.11569052783803326}\n",
      "{'loss': 0.607, 'learning_rate': 3.35e-05, 'grad_norm': 0.56640625, 'epoch': 0.1193058568329718}\n",
      "{'loss': 0.6258, 'learning_rate': 3.3e-05, 'grad_norm': 0.5546875, 'epoch': 0.12292118582791034}\n",
      "{'loss': 0.6156, 'learning_rate': 3.2500000000000004e-05, 'grad_norm': 0.63671875, 'epoch': 0.1265365148228489}\n",
      "{'loss': 0.6157, 'learning_rate': 3.2000000000000005e-05, 'grad_norm': 0.5546875, 'epoch': 0.1301518438177874}\n",
      "{'loss': 0.6201, 'learning_rate': 3.15e-05, 'grad_norm': 0.4921875, 'epoch': 0.13376717281272596}\n",
      "{'loss': 0.5866, 'learning_rate': 3.1e-05, 'grad_norm': 0.7265625, 'epoch': 0.1373825018076645}\n",
      "{'loss': 0.5913, 'learning_rate': 3.05e-05, 'grad_norm': 0.59765625, 'epoch': 0.14099783080260303}\n",
      "{'loss': 0.5931, 'learning_rate': 3e-05, 'grad_norm': 0.62890625, 'epoch': 0.14461315979754158}\n",
      "{'loss': 0.6094, 'learning_rate': 2.95e-05, 'grad_norm': 0.640625, 'epoch': 0.14822848879248013}\n",
      "{'loss': 0.6037, 'learning_rate': 2.9e-05, 'grad_norm': 0.57421875, 'epoch': 0.15184381778741865}\n",
      "{'loss': 0.6133, 'learning_rate': 2.8499999999999998e-05, 'grad_norm': 0.5859375, 'epoch': 0.1554591467823572}\n",
      "{'loss': 0.5938, 'learning_rate': 2.8000000000000003e-05, 'grad_norm': 0.59375, 'epoch': 0.15907447577729572}\n",
      "{'loss': 0.6115, 'learning_rate': 2.7500000000000004e-05, 'grad_norm': 0.69140625, 'epoch': 0.16268980477223427}\n",
      "{'loss': 0.5956, 'learning_rate': 2.7000000000000002e-05, 'grad_norm': 0.6640625, 'epoch': 0.16630513376717282}\n",
      "{'loss': 0.5798, 'learning_rate': 2.6500000000000004e-05, 'grad_norm': 0.62109375, 'epoch': 0.16992046276211134}\n",
      "{'loss': 0.6057, 'learning_rate': 2.6000000000000002e-05, 'grad_norm': 0.74609375, 'epoch': 0.1735357917570499}\n",
      "{'loss': 0.6069, 'learning_rate': 2.5500000000000003e-05, 'grad_norm': 0.59375, 'epoch': 0.17715112075198844}\n",
      "{'loss': 0.5835, 'learning_rate': 2.5e-05, 'grad_norm': 0.66015625, 'epoch': 0.18076644974692696}\n",
      "Saving model checkpoint to /home/ubuntu/environment/ml/qwen/checkpoint-500\n",
      "Model parallelism is enabled, saving the model sharded state dict instead of the full state dict.\n",
      "[2025-10-04 21:19:50.210: I neuronx_distributed/trainer/checkpoint.py:144] synced saving of checkpoint adapter_shards began\n",
      "[2025-10-04 21:19:50.371: I neuronx_distributed/trainer/checkpoint.py:192] synced saving of checkpoint adapter_shards completed\n",
      "[2025-10-04 21:19:50.372: I neuronx_distributed/trainer/checkpoint.py:256] no checkpoints to remove.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:485: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:485: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "{'loss': 0.6279, 'learning_rate': 2.45e-05, 'grad_norm': 0.69921875, 'epoch': 0.1843817787418655}\n",
      "{'loss': 0.5729, 'learning_rate': 2.4e-05, 'grad_norm': 0.59375, 'epoch': 0.18799710773680406}\n",
      "{'loss': 0.5851, 'learning_rate': 2.35e-05, 'grad_norm': 0.703125, 'epoch': 0.19161243673174258}\n",
      "{'loss': 0.6042, 'learning_rate': 2.3000000000000003e-05, 'grad_norm': 0.58984375, 'epoch': 0.19522776572668113}\n",
      "{'loss': 0.5612, 'learning_rate': 2.25e-05, 'grad_norm': 0.5859375, 'epoch': 0.19884309472161968}\n",
      "{'loss': 0.5727, 'learning_rate': 2.2000000000000003e-05, 'grad_norm': 0.7109375, 'epoch': 0.2024584237165582}\n",
      "{'loss': 0.5868, 'learning_rate': 2.15e-05, 'grad_norm': 0.64453125, 'epoch': 0.20607375271149675}\n",
      "{'loss': 0.5552, 'learning_rate': 2.1e-05, 'grad_norm': 0.6484375, 'epoch': 0.2096890817064353}\n",
      "{'loss': 0.5953, 'learning_rate': 2.05e-05, 'grad_norm': 0.63671875, 'epoch': 0.21330441070137382}\n",
      "{'loss': 0.589, 'learning_rate': 2e-05, 'grad_norm': 0.578125, 'epoch': 0.21691973969631237}\n",
      "{'loss': 0.5712, 'learning_rate': 1.9500000000000003e-05, 'grad_norm': 0.62890625, 'epoch': 0.2205350686912509}\n",
      "{'loss': 0.6063, 'learning_rate': 1.9e-05, 'grad_norm': 0.66015625, 'epoch': 0.22415039768618944}\n",
      "{'loss': 0.5976, 'learning_rate': 1.85e-05, 'grad_norm': 0.65234375, 'epoch': 0.227765726681128}\n",
      "{'loss': 0.5681, 'learning_rate': 1.8e-05, 'grad_norm': 0.66796875, 'epoch': 0.2313810556760665}\n",
      "{'loss': 0.5727, 'learning_rate': 1.75e-05, 'grad_norm': 0.56640625, 'epoch': 0.23499638467100506}\n",
      "{'loss': 0.5796, 'learning_rate': 1.7000000000000003e-05, 'grad_norm': 0.77734375, 'epoch': 0.2386117136659436}\n",
      "{'loss': 0.6014, 'learning_rate': 1.65e-05, 'grad_norm': 0.703125, 'epoch': 0.24222704266088213}\n",
      "{'loss': 0.608, 'learning_rate': 1.6000000000000003e-05, 'grad_norm': 0.59765625, 'epoch': 0.24584237165582068}\n",
      "{'loss': 0.5956, 'learning_rate': 1.55e-05, 'grad_norm': 0.76953125, 'epoch': 0.24945770065075923}\n",
      "{'loss': 0.6057, 'learning_rate': 1.5e-05, 'grad_norm': 0.625, 'epoch': 0.2530730296456978}\n",
      "{'loss': 0.583, 'learning_rate': 1.45e-05, 'grad_norm': 0.65625, 'epoch': 0.25668835864063627}\n",
      "{'loss': 0.5922, 'learning_rate': 1.4000000000000001e-05, 'grad_norm': 0.8046875, 'epoch': 0.2603036876355748}\n",
      "{'loss': 0.5784, 'learning_rate': 1.3500000000000001e-05, 'grad_norm': 0.67578125, 'epoch': 0.26391901663051337}\n",
      "{'loss': 0.5647, 'learning_rate': 1.3000000000000001e-05, 'grad_norm': 0.75390625, 'epoch': 0.2675343456254519}\n",
      "{'loss': 0.5591, 'learning_rate': 1.25e-05, 'grad_norm': 0.64453125, 'epoch': 0.27114967462039047}\n",
      "{'loss': 0.5794, 'learning_rate': 1.2e-05, 'grad_norm': 0.62109375, 'epoch': 0.274765003615329}\n",
      "{'loss': 0.5825, 'learning_rate': 1.1500000000000002e-05, 'grad_norm': 0.703125, 'epoch': 0.2783803326102675}\n",
      "{'loss': 0.626, 'learning_rate': 1.1000000000000001e-05, 'grad_norm': 0.828125, 'epoch': 0.28199566160520606}\n",
      "{'loss': 0.5751, 'learning_rate': 1.05e-05, 'grad_norm': 0.58984375, 'epoch': 0.2856109906001446}\n",
      "{'loss': 0.5866, 'learning_rate': 1e-05, 'grad_norm': 0.6328125, 'epoch': 0.28922631959508316}\n",
      "{'loss': 0.543, 'learning_rate': 9.5e-06, 'grad_norm': 0.58984375, 'epoch': 0.2928416485900217}\n",
      "{'loss': 0.5908, 'learning_rate': 9e-06, 'grad_norm': 0.6484375, 'epoch': 0.29645697758496026}\n",
      "{'loss': 0.596, 'learning_rate': 8.500000000000002e-06, 'grad_norm': 0.671875, 'epoch': 0.30007230657989875}\n",
      "{'loss': 0.5656, 'learning_rate': 8.000000000000001e-06, 'grad_norm': 0.69140625, 'epoch': 0.3036876355748373}\n",
      "{'loss': 0.5717, 'learning_rate': 7.5e-06, 'grad_norm': 0.7109375, 'epoch': 0.30730296456977585}\n",
      "{'loss': 0.552, 'learning_rate': 7.000000000000001e-06, 'grad_norm': 0.61328125, 'epoch': 0.3109182935647144}\n",
      "{'loss': 0.5696, 'learning_rate': 6.5000000000000004e-06, 'grad_norm': 0.64453125, 'epoch': 0.31453362255965295}\n",
      "{'loss': 0.5694, 'learning_rate': 6e-06, 'grad_norm': 0.71875, 'epoch': 0.31814895155459144}\n",
      "{'loss': 0.5799, 'learning_rate': 5.500000000000001e-06, 'grad_norm': 0.6484375, 'epoch': 0.32176428054953}\n",
      "{'loss': 0.5572, 'learning_rate': 5e-06, 'grad_norm': 0.70703125, 'epoch': 0.32537960954446854}\n",
      "{'loss': 0.5815, 'learning_rate': 4.5e-06, 'grad_norm': 0.6953125, 'epoch': 0.3289949385394071}\n",
      "{'loss': 0.5654, 'learning_rate': 4.000000000000001e-06, 'grad_norm': 0.65625, 'epoch': 0.33261026753434564}\n",
      "{'loss': 0.5745, 'learning_rate': 3.5000000000000004e-06, 'grad_norm': 0.69140625, 'epoch': 0.3362255965292842}\n",
      "{'loss': 0.572, 'learning_rate': 3e-06, 'grad_norm': 0.75390625, 'epoch': 0.3398409255242227}\n",
      "{'loss': 0.577, 'learning_rate': 2.5e-06, 'grad_norm': 0.62109375, 'epoch': 0.34345625451916123}\n",
      "{'loss': 0.5559, 'learning_rate': 2.0000000000000003e-06, 'grad_norm': 0.66796875, 'epoch': 0.3470715835140998}\n",
      "{'loss': 0.5726, 'learning_rate': 1.5e-06, 'grad_norm': 0.671875, 'epoch': 0.35068691250903833}\n",
      "{'loss': 0.5606, 'learning_rate': 1.0000000000000002e-06, 'grad_norm': 0.640625, 'epoch': 0.3543022415039769}\n",
      "{'loss': 0.6042, 'learning_rate': 5.000000000000001e-07, 'grad_norm': 0.6328125, 'epoch': 0.3579175704989154}\n",
      "{'loss': 0.5545, 'learning_rate': 0.0, 'grad_norm': 0.66796875, 'epoch': 0.3615328994938539}\n",
      "Saving model checkpoint to /home/ubuntu/environment/ml/qwen/checkpoint-1000\n",
      "Model parallelism is enabled, saving the model sharded state dict instead of the full state dict.\n",
      "[2025-10-04 21:28:11.727: I neuronx_distributed/trainer/checkpoint.py:144] synced saving of checkpoint adapter_shards began\n",
      "[2025-10-04 21:28:11.885: I neuronx_distributed/trainer/checkpoint.py:192] synced saving of checkpoint adapter_shards completed\n",
      "[2025-10-04 21:28:11.886: I neuronx_distributed/trainer/checkpoint.py:256] no checkpoints to remove.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1035.6832, 'train_samples_per_second': 1.931, 'train_steps_per_second': 0.966, 'train_loss': 0.6340726699829101, 'epoch': 0.3615328994938539}\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "Consolidating LoRA adapter shards\n",
      "Merging LoRA adapter shards into base model\n",
      "Saving merged model to /home/ubuntu/environment/ml/qwen/merged_model\n",
      "Saving tokenizer to /home/ubuntu/environment/ml/qwen/merged_model\n",
      "Merged model config:\n",
      "Qwen3Model(\n",
      "  (embed_tokens): Embedding(151936, 2048)\n",
      "  (layers): ModuleList(\n",
      "    (0-27): 28 x Qwen3DecoderLayer(\n",
      "      (self_attn): Qwen3Attention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "      )\n",
      "      (mlp): Qwen3MLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
      "        (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "  (rotary_emb): Qwen3RotaryEmbedding()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 \\\n",
    "finetune_llama.py \\\n",
    "--bf16 True --dataloader_drop_last True --disable_tqdm True --gradient_accumulation_steps 1 \\\n",
    "--gradient_checkpointing True --learning_rate 5e-05 --logging_steps 10 --lora_alpha 32 \\\n",
    "--lora_dropout 0.05 --lora_r 16 --max_steps 1000 \\\n",
    "--model_id Qwen/Qwen3-1.7B --output_dir ~/environment/ml/qwen \\\n",
    "--per_device_train_batch_size 2 --tensor_parallel_size 2 \\\n",
    "--tokenizer_id Qwen/Qwen3-1.7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilation\n",
    "\n",
    "Since we have everything installed locally, we don't need to use a training job like on SageMaker.  We can just call the optimum-cli command directly.\n",
    "\n",
    "The training process runs a merge script at the end, so we are using the output_dir and adding a merged_model path and then saving our compiled model into the compiled_model path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/commands/env.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/commands/env.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/kvcache/kv_cache_manager.py:24: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..attention.gqa import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/llama/modeling_llama.py:45: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..backend.modules.attention.attention_base import NeuronAttentionBase\n",
      "INFO:Neuron:Generating HLOs for the following models: ['context_encoding_model', 'token_generation_model']\n",
      "[2025-09-04 13:24:24.405: I neuronx_distributed/parallel_layers/parallel_state.py:628] > initializing tensor model parallel with size 2\n",
      "[2025-09-04 13:24:24.405: I neuronx_distributed/parallel_layers/parallel_state.py:629] > initializing pipeline model parallel with size 1\n",
      "[2025-09-04 13:24:24.405: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing context model parallel with size 1\n",
      "[2025-09-04 13:24:24.405: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing data parallel with size 1\n",
      "[2025-09-04 13:24:24.405: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing world size to 2\n",
      "[2025-09-04 13:24:24.406: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x77de24b44790>, 'Ascending Ring PG Group')>\n",
      "[2025-09-04 13:24:24.407: I neuronx_distributed/parallel_layers/parallel_state.py:668] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1]]\n",
      "[2025-09-04 13:24:24.407: I neuronx_distributed/parallel_layers/parallel_state.py:669] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1]]\n",
      "[2025-09-04 13:24:24.407: I neuronx_distributed/parallel_layers/parallel_state.py:670] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1]]\n",
      "[2025-09-04 13:24:24.407: I neuronx_distributed/parallel_layers/parallel_state.py:671] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1]]\n",
      "[2025-09-04 13:24:24.407: I neuronx_distributed/parallel_layers/parallel_state.py:672] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1]]\n",
      "[2025-09-04 13:24:24.407: I neuronx_distributed/parallel_layers/parallel_state.py:673] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1]]\n",
      "INFO:Neuron:Generating 1 hlos for key: context_encoding_model\n",
      "INFO:Neuron:Started loading module context_encoding_model\n",
      "INFO:Neuron:Finished loading module context_encoding_model in 0.1381816864013672 seconds\n",
      "INFO:Neuron:generating HLO: context_encoding_model, input example shape = torch.Size([1, 512])\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:485: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/generation/sampling.py:299: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs_soft_max, dim=dim, on_cpu=self.on_cpu)\n",
      "Add predicate {{0,+,-1}<i0=[0:128:1]>,+,0}<i1=[0:2048:1]>\n",
      "start lb and ub of  {0,+,-1}<i0=[0:128:1]> is 0 0\n",
      "Add predicate {{255,+,0}<i0=[0:128:1]>,+,-1}<i1=[0:2048:1]>\n",
      "start lb and ub of  {{255,+,0}<i0=[0:128:1]>,+,-1}<i1=[0:2048:1]> is 255 255\n",
      "before build_invert_ranges alive full {\n",
      "  0 <= i1=[0:2048:1] <= 2047; alive full {\n",
      "    0 <= i1=[0:2048:1] <= 2047; 1 <= i0=[0:128:1] <= 127; alive leaf\n",
      "  }\n",
      "  256 <= i1=[0:2048:1] <= 2047; alive {\n",
      "    256 <= i1=[0:2048:1] <= 2047; 0 <= i0=[0:128:1] <= 127; alive full leaf\n",
      "  }\n",
      "}\n",
      "generated domains alive full {\n",
      "  0 <= i1=[0:2048:1] <= 255; alive {\n",
      "    0 <= i1=[0:2048:1] <= 255; 0 <= i0=[0:128:1] <= 0; alive leaf\n",
      "  }\n",
      "}\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/generation/sampling.py:262: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.on_cpu)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:289: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1, 512]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:289: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.\n",
      "  warnings.warn(\n",
      "INFO:Neuron:Finished generating HLO for context_encoding_model in 2.587005853652954 seconds, input example shape = torch.Size([1, 512])\n",
      "INFO:Neuron:Generating 1 hlos for key: token_generation_model\n",
      "INFO:Neuron:Started loading module token_generation_model\n",
      "INFO:Neuron:Finished loading module token_generation_model in 0.11980652809143066 seconds\n",
      "INFO:Neuron:generating HLO: token_generation_model, input example shape = torch.Size([1, 1])\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:485: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/generation/sampling.py:299: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs_soft_max, dim=dim, on_cpu=self.on_cpu)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/generation/sampling.py:262: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.on_cpu)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:289: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.\n",
      "  warnings.warn(\n",
      "INFO:Neuron:Finished generating HLO for token_generation_model in 2.3019745349884033 seconds, input example shape = torch.Size([1, 1])\n",
      "INFO:Neuron:Generated all HLOs in 5.210456848144531 seconds\n",
      "INFO:Neuron:Starting compilation for the priority HLO\n",
      "INFO:Neuron:'token_generation_model' is the priority model with bucket rank 0\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/libneuronxla/neuron_cc_wrapper.py:283: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.\n",
      "  warnings.warn(SyntaxWarning(\n",
      "Fetched cached neuronxcc-2.20.9961.0+0acef03a/MODULE_de733c6f96020a1e5f56+a9d440f5/model.neff from aws-neuron/optimum-neuron-cache\n",
      "2025-09-04 13:24:30.000210:  255113  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_de733c6f96020a1e5f56+a9d440f5/model.neff\n",
      "Fetched cached neuronxcc-2.20.9961.0+0acef03a/MODULE_de733c6f96020a1e5f56+a9d440f5/wrapped_neff.hlo from aws-neuron/optimum-neuron-cache\n",
      "INFO:Neuron:Done compilation for the priority HLO in 0.8264033794403076 seconds\n",
      "INFO:Neuron:Updating the hlo module with optimized layout\n",
      "INFO:Neuron:Done optimizing weight layout for all HLOs in 0.5720491409301758 seconds\n",
      "INFO:Neuron:Starting compilation for all HLOs\n",
      "INFO:Neuron:Neuron compiler flags: --auto-cast=none --model-type=transformer --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' -O2  --lnc=1 --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk0/log-neuron-cc.txt\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/libneuronxla/neuron_cc_wrapper.py:245: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.\n",
      "  warnings.warn(SyntaxWarning(\n",
      "Fetched cached neuronxcc-2.20.9961.0+0acef03a/MODULE_057bc784fc164fb34d3e+ed72d204/model.neff from aws-neuron/optimum-neuron-cache\n",
      "2025-09-04 13:24:31.000549:  255113  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_057bc784fc164fb34d3e+ed72d204/model.neff\n",
      "INFO:Neuron:Finished Compilation for all HLOs in 0.5351839065551758 seconds\n",
      "Fetched cached neuronxcc-2.20.9961.0+0acef03a/MODULE_e3031c3c114f7c905db7+ae6a382b/model.neff from aws-neuron/optimum-neuron-cache\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_e3031c3c114f7c905db7+ae6a382b/wrapped_neff.hlo not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "INFO:Neuron:Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_e3031c3c114f7c905db7+ae6a382b/model.neff\n",
      "INFO:Neuron:Done preparing weight layout transformation\n",
      "INFO:Neuron:Finished building model in 7.8153510093688965 seconds\n",
      "Configuration saved in /home/ubuntu/environment/ml/qwen/compiled_model/neuron_config.json\n",
      "INFO:Neuron:Sharding Weights for ranks: 0...1\n",
      "[2025-09-04 13:24:32.363: I neuronx_distributed/parallel_layers/parallel_state.py:628] > initializing tensor model parallel with size 2\n",
      "[2025-09-04 13:24:32.364: I neuronx_distributed/parallel_layers/parallel_state.py:629] > initializing pipeline model parallel with size 1\n",
      "[2025-09-04 13:24:32.364: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing context model parallel with size 1\n",
      "[2025-09-04 13:24:32.364: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing data parallel with size 1\n",
      "[2025-09-04 13:24:32.364: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing world size to 2\n",
      "[2025-09-04 13:24:32.365: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x77de24b44790>, 'Ascending Ring PG Group')>\n",
      "[2025-09-04 13:24:32.365: I neuronx_distributed/parallel_layers/parallel_state.py:668] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1]]\n",
      "[2025-09-04 13:24:32.365: I neuronx_distributed/parallel_layers/parallel_state.py:669] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1]]\n",
      "[2025-09-04 13:24:32.365: I neuronx_distributed/parallel_layers/parallel_state.py:670] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1]]\n",
      "[2025-09-04 13:24:32.365: I neuronx_distributed/parallel_layers/parallel_state.py:671] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1]]\n",
      "[2025-09-04 13:24:32.365: I neuronx_distributed/parallel_layers/parallel_state.py:672] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1]]\n",
      "[2025-09-04 13:24:32.366: I neuronx_distributed/parallel_layers/parallel_state.py:673] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1]]\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/trace/trace.py:640: UserWarning: Removing redundant keys from checkpoint: []\n",
      "  warnings.warn(f\"Removing redundant keys from checkpoint: {keys_to_delete}\")\n",
      "INFO:Neuron:Done Sharding weights in 33.1569037700101\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export neuron --model /home/ubuntu/environment/ml/qwen/merged_model --task text-generation --sequence_length 512 --batch_size 1 --num_cores 2 /home/ubuntu/environment/ml/qwen/compiled_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "We will install the Optimum Neuron vllm option.  Then, run inference using the compiled model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install optimum-neuron[vllm]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-04 13:30:37 [config.py:841] This model supports multiple tasks: {'classify', 'embed', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 09-04 13:30:37 [config.py:1472] Using max model len 2048\n",
      "INFO 09-04 13:30:37 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2) with config: model='/home/ubuntu/environment/ml/qwen/compiled_model', speculative_config=None, tokenizer='/home/ubuntu/environment/ml/qwen/compiled_model', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cpu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/home/ubuntu/environment/ml/qwen/compiled_model, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":1,\"local_cache_dir\":null}, use_cached_outputs=False, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Neuron:Loading sharded checkpoint from /home/ubuntu/environment/ml/qwen/compiled_model/checkpoint/weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-04 13:30:39 [config.py:4861] Current vLLM config is not set.\n",
      "INFO 09-04 13:30:39 [executor_base.py:113] # neuron blocks: 2, # CPU blocks: 0\n",
      "INFO 09-04 13:30:39 [executor_base.py:118] Maximum concurrency for 2048 tokens per request: 2.00x\n",
      "INFO 09-04 13:30:39 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 0.00 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453fc3eae52742329f88dbc2afbfee14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b15fe6455a942d497e089fe61a4a499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################################\n",
      "Prompt: '\\n<|system|>\\nYou are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE management (department_id VARCHAR); CREATE TABLE department (department_id VARCHAR)</s>\\n<|user|>\\nHow many departments are led by heads who are not mentioned?</s>\\n<|assistant|>\\n', \n",
      "\n",
      " Generated text: 'SELECT COUNT(*) FROM management WHERE department_id NOT IN (SELECT department_id FROM department);' \n",
      "\n",
      "Prompt: '\\n<|system|>\\nYou are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE courses (course_name VARCHAR, course_id VARCHAR); CREATE TABLE \\nstudent_course_registrations (student_id VARCHAR, course_id VARCHAR)</s>\\n<|user|>\\nWhat are the ids of all students for courses and what are the names of those courses?</s>\\n<|assistant|>\\n', \n",
      "\n",
      " Generated text: 'SELECT T2.course_id, T2.course_name FROM student_course_registrations AS T1 JOIN courses AS T2 ON T1.course_id = T2.course_id;' \n",
      "\n",
      "Prompt: '\\n<|system|>\\nYou are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_9 (wins INTEGER, year VARCHAR, team VARCHAR, points VARCHAR)</s>\\n<|user|>\\nWhich highest wins number had Kawasaki as a team, 95 points, and a year prior to 1981?</s>\\n<|assistant|>\\n', \n",
      "\n",
      " Generated text: '<|start_of_turn|>\\n<s>\\nSELECT MAX(wins) FROM table_name_9 WHERE team = \"kawasaki\" AND points = 95 AND year < 1981;' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "llm = LLM(\n",
    "    model=\"/home/ubuntu/environment/ml/qwen/compiled_model\", #local compiled model\n",
    "    max_num_seqs=1,\n",
    "    max_model_len=2048,\n",
    "    device=\"neuron\",\n",
    "    tensor_parallel_size=2,\n",
    "    override_neuron_config={})\n",
    "example1=\"\"\"\n",
    "<|im_start|>system\n",
    "You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "CREATE TABLE management (department_id VARCHAR); CREATE TABLE department (department_id VARCHAR)<|im_end|>\n",
    "<|im_start|>user\n",
    "How many departments are led by heads who are not mentioned?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "example2=\"\"\"\n",
    "<|im_start|>system\n",
    "You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "CREATE TABLE courses (course_name VARCHAR, course_id VARCHAR); CREATE TABLE student_course_registrations (student_id VARCHAR, course_id VARCHAR)<|im_end|>\n",
    "<|im_start|>user\n",
    "What are the ids of all students for courses and what are the names of those courses?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "example3=\"\"\"\n",
    "<|im_start|>system\n",
    "You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "CREATE TABLE table_name_9 (wins INTEGER, year VARCHAR, team VARCHAR, points VARCHAR)<|im_end|>\n",
    "<|im_start|>user\n",
    "Which highest wins number had Kawasaki as a team, 95 points, and a year prior to 1981?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "prompts = [\n",
    "    example1,\n",
    "    example2,\n",
    "    example3\n",
    "]\n",
    "\n",
    "sampling_params = SamplingParams(max_tokens=2048, temperature=0.8)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "print(\"#########################################################\")\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, \\n\\n Generated text: {generated_text!r} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
